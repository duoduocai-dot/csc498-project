{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87085ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, observation_dim, params = None, action_bounds = None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        return self.act(obs)\n",
    "    \n",
    "\n",
    "# code for A3    \n",
    "class DQN(Agent):\n",
    "    def __init__(self, observation_dim, action_dim, gamma=0.9):   \n",
    "        self.actions = action_dim  # 3\n",
    "        self.obs_dim = observation_dim  # 6\n",
    "\n",
    "        # Q(x,x|w)\n",
    "        # q value is determined by s and a, so if input s, gives the values at each action\n",
    "\n",
    "        # input (x, obs_dim), output (x, actions)\n",
    "        self.q  = nn.Sequential(\n",
    "            nn.Linear(self.obs_dim, 32),  # input (x, obs_dim), output: (x, 32)\n",
    "            nn.ReLU(),                    # convert negs to 0\n",
    "            nn.Linear(32,32),             # input (y, 32), output (y, 32) \n",
    "            nn.ReLU(),                  \n",
    "            nn.Linear(32, self.actions)   # input (z, 32), output: (z, actions)\n",
    "        ).double()                        # x = y=z\n",
    "\n",
    "        # Q(x,x,w-)\n",
    "        self.q_target  = nn.Sequential(\n",
    "            nn.Linear(self.obs_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.actions)\n",
    "        ).double()\n",
    "\n",
    "        # we are doing gradient descent WRT self.optim corresponding q\n",
    "        self.optim = torch.optim.Adam(self.q.parameters(), lr=1e-4)\n",
    "        self.N_STEPS = 1000\n",
    "        self.BATCH_SIZE = 512\n",
    "        self.N_EPOCHS = 200\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_target(self, states, rewards):\n",
    "        \"\"\"\n",
    "        states: torch.Tensor of size (batch, obs_dim) with s' from the dataset\n",
    "        rewards: torch.Tensor of size (batch) with single step rewards (float)\n",
    "        \n",
    "        returns torch.Tensor of size (batch) with the 1-step Q learning target\n",
    "        \"\"\"\n",
    "        # states:  batch_next_states (512, obs_dim)\n",
    "        # rewards: batch_rewards (512, 1)\n",
    "        # just input batch of \n",
    "        # s' to Q(s', a'; w-) and choose the max one in the output tensor\n",
    "        # y_i<- r_i + gamma*max_a'(Q(s', a'; w-))\n",
    "        with torch.no_grad():\n",
    "            act_val = self.q_target(states)\n",
    "            opt_val,_ = torch.max(act_val,dim=1)\n",
    "            y = torch.add(torch.squeeze(rewards), self.gamma*opt_val)\n",
    "        return y\n",
    "\n",
    "    def loss(self, states, actions, target):\n",
    "        \"\"\"\n",
    "        states: torch.Tensor of size (batch, obs_dim) with s from the dataset\n",
    "        actions: torch.Tensor of size (batch, 1) with action from the dataset\n",
    "        target: torch.Tensor of size (batch) with computed target (see self.compute_target)\n",
    "        \n",
    "        returns torch.Tensor of size (1) with squared Q error\n",
    "        \n",
    "        Hint: you will need the torch.gather function\n",
    "        \"\"\"\n",
    "        act_val = self.q(states)\n",
    "        act_idx = torch.squeeze(actions)\n",
    "        exp_val = torch.squeeze(act_val.gather(1, act_idx.view(-1,1)))\n",
    "        res = nn.functional.mse_loss(exp_val, target, size_average=None, reduce=None, reduction='mean')\n",
    "        return res\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        \"\"\"\n",
    "        states: np.array of size (obs_dim,) with the current state\n",
    "        returns np.array of size (1,) with the optimal action\n",
    "        \"\"\"\n",
    "        # epsilon = 0.05 unstable, 0.1 unstable\n",
    "        epsilon = 0.15\n",
    "        state = torch.from_numpy(state).view(1,-1)\n",
    "        state = state.double()\n",
    "        tmp = self.q(state)[0]\n",
    "        act_val = tmp.detach().numpy()  # act_val: (action_dim,)\n",
    "        act_idx = np.array(range(self.actions))\n",
    "        \n",
    "        greedy_prob_func = np.vectorize(lambda x: 1-epsilon+epsilon/self.actions \\\n",
    "            if x == np.argmax(act_val) else epsilon/self.actions)\n",
    "        next_act_dist = greedy_prob_func(act_idx)\n",
    "        next_act = np.random.choice(act_idx, p=next_act_dist)\n",
    "        return next_act\n",
    "\n",
    "    def train_epoch(self, states, actions, reward, next_states):\n",
    "        # Do not modify\n",
    "        #  states and next_states: (100,100,obs_dim)\n",
    "        # actions:(100,100,1), rewards:(100,100,1)\n",
    "        num_runs = states.shape[0]\n",
    "        len_runs = states.shape[1]\n",
    "        losses = []\n",
    "\n",
    "        for i in range(self.N_STEPS):   # 1000\n",
    "            # sample batch_x of shape (512,), each within (0, 100)\n",
    "            batch_x = np.random.randint(num_runs, size=(self.BATCH_SIZE,))\n",
    "            # sample batch_y of shape (512,), each within (0, 100)\n",
    "            batch_y = np.random.randint(len_runs, size=(self.BATCH_SIZE,))\n",
    "            # batch_states (512, obs_dim), batch_actions (512, 1), \n",
    "            # batch_rewards (512, 1), batch_next_states (512, obs_dim)\n",
    "            batch_states = torch.from_numpy(states[batch_x,batch_y])\n",
    "            batch_actions = torch.from_numpy(actions[batch_x,batch_y]).to(int)\n",
    "            batch_rewards = torch.from_numpy(reward[batch_x,batch_y])\n",
    "            batch_next_states = torch.from_numpy(next_states[batch_x,batch_y])\n",
    "            # target: (batch)\n",
    "            target = self.compute_target(batch_next_states, batch_rewards)\n",
    "\n",
    "            # loss: (1)\n",
    "            loss = self.loss(batch_states, batch_actions, target)\n",
    "             \n",
    "            # clears x.grad for every parameter x in the optimizer\n",
    "            # doing this before loss.backward() avoids accumulate the\n",
    "            #  gradients from multiple passes\n",
    "            self.optim.zero_grad()\n",
    "\n",
    "            # loss.backward() computes dloss/dx for every parameter x which has\n",
    "            # requires_grad=True, those are accumulated into x.grad for every\n",
    "            # parameter x. x.grad += dloss/dx\n",
    "            loss.backward()\n",
    "                                                \n",
    "            # optimizer.step updates the value of x using x.grad.\n",
    "            self.optim.step()\n",
    "            losses.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            # Loads a modelâ€™s parameter dictionary using a deserialized state_dict.\n",
    "            self.q_target.load_state_dict(self.q.state_dict())\n",
    "        return losses\n",
    "        \n",
    "    def train(self, task):\n",
    "        # Do not modify\n",
    "        losses = []\n",
    "        # states: (100,101,self.obs_dim), actions:(100,100,1), rewards:(100,100,1)\n",
    "        # sampled randomly meaning the actions are taken randomly\n",
    "        states, actions, rewards = self.collect_data(task, random=True)\n",
    "\n",
    "        for i in range(self.N_EPOCHS):\n",
    "            # choice of policy is based on __call__()\n",
    "            states, actions, rewards = self.collect_data(task, random=False)\n",
    "            # first: for each run, select first 100 steps\n",
    "            # last: for each run, select last 100 steps \n",
    "            epoch_losses = self.train_epoch(states[:, :-1], actions, rewards, states[:, 1:])\n",
    "            losses += epoch_losses\n",
    "        return losses\n",
    "            \n",
    "    def collect_data(self, task, random=False):\n",
    "        # Do not modify\n",
    "        rewards = np.zeros((100,100,1))\n",
    "        states = np.zeros((100,101,self.obs_dim))\n",
    "        actions = np.zeros((100,100,1))\n",
    "\n",
    "        for run in range(100):\n",
    "            obs = task.reset()\n",
    "            for step in range(100):\n",
    "                states[run, step] = obs\n",
    "                act = np.random.choice([self(obs), np.random.randint(self.actions)], p=[0.9, 0.1])\n",
    "                if random:\n",
    "                    act = np.random.randint(self.actions)\n",
    "                obs, rew, done, info = task.step(act)\n",
    "                rewards[run, step] = rew    # reward is given by the dynamics\n",
    "                actions[run, step] = act    # record that action\n",
    "#             print(\"rewards:\", rewards.shape, rewards)\n",
    "            states[run, -1] = obs   # final state\n",
    "            \n",
    "        print(f\"Average return in training: {np.mean(rewards)}\")\n",
    "        return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "breakout_env = Breakout()\n",
    "breakout_env.main()\n",
    "breakout_env.make()\n",
    "\n",
    "agent = DQN(breakout_env.observation_space, breakout_env.action_space)\n",
    "\n",
    "losses = agent.train(breakout_env)\n",
    "plt.plot(np.arange(len(losses)), losses)\n",
    "plt.savefig('3a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
