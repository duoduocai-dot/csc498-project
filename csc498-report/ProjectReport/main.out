\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Environment}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Rewards}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Environment variables, functions}{section.2}% 4
\BOOKMARK [1][-]{section.3}{Algorithims}{}% 5
\BOOKMARK [2][-]{subsection.3.1}{Tabular Algorithms}{section.3}% 6
\BOOKMARK [3][-]{subsubsection.3.1.1}{Tabular Q-Learning}{subsection.3.1}% 7
\BOOKMARK [3][-]{subsubsection.3.1.2}{Tabular Double Q-Learning}{subsection.3.1}% 8
\BOOKMARK [3][-]{subsubsection.3.1.3}{Tabular Sarsa}{subsection.3.1}% 9
\BOOKMARK [3][-]{subsubsection.3.1.4}{Training}{subsection.3.1}% 10
\BOOKMARK [2][-]{subsection.3.2}{Tabular algorithms Analysis}{section.3}% 11
\BOOKMARK [3][-]{subsubsection.3.2.1}{Performance}{subsection.3.2}% 12
\BOOKMARK [3][-]{subsubsection.3.2.2}{Comparing Tabular Algorithms}{subsection.3.2}% 13
\BOOKMARK [3][-]{subsubsection.3.2.3}{Parameter and Hyperparameter Choice}{subsection.3.2}% 14
\BOOKMARK [3][-]{subsubsection.3.2.4}{Q-Learning maximization problems}{subsection.3.2}% 15
\BOOKMARK [3][-]{subsubsection.3.2.5}{Generalization}{subsection.3.2}% 16
\BOOKMARK [3][-]{subsubsection.3.2.6}{Which Layout is best to train on?}{subsection.3.2}% 17
\BOOKMARK [2][-]{subsection.3.3}{Policy Gradient}{section.3}% 18
\BOOKMARK [3][-]{subsubsection.3.3.1}{Reward schema problems}{subsection.3.3}% 19
\BOOKMARK [3][-]{subsubsection.3.3.2}{Gradients getting trapped}{subsection.3.3}% 20
\BOOKMARK [2][-]{subsection.3.4}{Deep Recurrent Q-Network}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.5}{Conclusion}{section.3}% 22
