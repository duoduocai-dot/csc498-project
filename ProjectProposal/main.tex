\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{csc498project}



% Short headings should be running head and authors last names

\ShortHeadings{Atari Breakout Reinforcement Learning Environment}{Your names}
\firstpageno{1}


\title{Atari Breakout Reinforcement Learning Environment}

\author{\name Haider Sajjad \email haider.sajjad@mail.utoronto.ca \\
       \addr 1004076251\\
       \AND
      \name Weiyu Li \email weiyu.li@mail.utoronto.ca \\
       \addr 1003765981}

\begin{document}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\textit{
Atari Breakout environment implementation and training an agent using multiple algorithms over a generated environment (changing brick layouts)}

\end{abstract}

\section{Problem Statement}

We intend to implement an atari breakout game environment and train a reinforcement learning agent to play the game. The environment will be built over-top a pygame class of the game and we add the necessary environment variables and functions. We will implement the agent to train with different algorithms taught in class and beyond (Q-learning, SARSA, TD(0)/TD(n)-learning,DQN) and compare performance. We will also aim to train the agent generally, to play atari breakout across different levels (by changing brick layout) and achieve a high score, or win the game. We will also compare the performance of the agent over multiple levels by training it over different levels (by generating different brick layouts in the environment), compared to training the agent on a single environment.
\section{Motivation and Impact}
We want to compare the strengths and weaknesses of different RL algorithms in our case of atari breakout, and analyze how well our agent generalizes across different environments (levels) of atari breakout. Our results may motivate strategies on more complex games by projecting our strategies on to them. This may result in more complex and better preforming ai components in video games in general.
\section{Intuition}
How we will do this is by implementing a RL environment structure (done, step, rewards, actions, reset) by implementing its variables and functions into an already built breakout game class made using pygame. A game state in our environment will have 7 components: paddle x location,  ball (x,y) location, ball (x,y) speed, bricks left, current score. \\
The general reward mechanisms will be: +10 if ball hits paddle,  -10 if ball misses paddle, +20 when ball hits brick, +100 when all bricks are destroyed. We will experiment with these to get best performance. \\
There are 3 actions the agent can take: stay still, move left, move right.
\\\\
We will implement all the algorithms we learned in class (that are applicable) and compare there performance. We will attempt to train over both generated environments (changing brick layouts) and a stationary one, and see how this affects final agent performance. We will also test our trained agent across multiple levels of breakout to analyze how well our agent learned tasks generally.



% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\bibliography{bibliography}

\end{document}